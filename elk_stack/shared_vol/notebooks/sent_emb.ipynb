{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import pytorch_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65312/2550429865.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_trunc['stt_out'] = pd.Series(all_stt)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "      <th>stt_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382934.mp3</td>\n",
       "      <td>Een daadwerkelijke keuzevrijheid voor ouderen,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>een daadwerkelijke keuzevrijheid voor ouderen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382935.mp3</td>\n",
       "      <td>Elke kandidaat-lidstaat moet op zijn eigen mer...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elke kandidaat dit staat moet op zijn eigen wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382936.mp3</td>\n",
       "      <td>Het verslag legt sterke nadruk op het nauwe ve...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>het verslag legt sterke nadruk op het nauwe ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382937.mp3</td>\n",
       "      <td>Wij openen nu het algemeen debat.</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we openen nu het algemeen debat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382938.mp3</td>\n",
       "      <td>Die fase is gebaseerd op de testcyclus van per...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>die fase is gebaseerd op de test van personena...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           client_id  \\\n",
       "0  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "1  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "2  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "3  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "4  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "\n",
       "                           path  \\\n",
       "0  common_voice_nl_30382934.mp3   \n",
       "1  common_voice_nl_30382935.mp3   \n",
       "2  common_voice_nl_30382936.mp3   \n",
       "3  common_voice_nl_30382937.mp3   \n",
       "4  common_voice_nl_30382938.mp3   \n",
       "\n",
       "                                            sentence  up_votes  down_votes  \\\n",
       "0  Een daadwerkelijke keuzevrijheid voor ouderen,...         2           0   \n",
       "1  Elke kandidaat-lidstaat moet op zijn eigen mer...         2           0   \n",
       "2  Het verslag legt sterke nadruk op het nauwe ve...         2           0   \n",
       "3                  Wij openen nu het algemeen debat.         4           0   \n",
       "4  Die fase is gebaseerd op de testcyclus van per...         4           0   \n",
       "\n",
       "   age gender                accents locale  segment  \\\n",
       "0  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "1  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "2  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "3  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "4  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "\n",
       "                                             stt_out  \n",
       "0  een daadwerkelijke keuzevrijheid voor ouderen ...  \n",
       "1  elke kandidaat dit staat moet op zijn eigen wo...  \n",
       "2  het verslag legt sterke nadruk op het nauwe ve...  \n",
       "3                    we openen nu het algemeen debat  \n",
       "4  die fase is gebaseerd op de test van personena...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"../../third_party_data/train.tsv\"\n",
    "df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "\n",
    "df_trunc = df.iloc[0:10, :]\n",
    "all_stt = []\n",
    "for fname in df.path.values:\n",
    "    with open(f\"../../third_party_data/cv_nl_stt/{fname}.json\", \"r+\") as f:\n",
    "        stt_out = json.load(f)\n",
    "        all_stt.append(stt_out['results']['channels'][0]['alternatives'][0]['transcript'])\n",
    "df_trunc['stt_out'] = pd.Series(all_stt)\n",
    "df_trunc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentences using sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a multi-linual model compatible with sentence transformers\n",
    "model1 = SentenceTransformer(\"../models/paraphrase-MiniLM-L6-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9802])\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(df_trunc.sentence):\n",
    "    embed_sent = model1.encode(sent)\n",
    "    embed_stt = model1.encode(df_trunc.stt_out.iloc[i])\n",
    "    print(pytorch_cos_sim(embed_sent, embed_stt)[0].cpu())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentences using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def get_embeddings(sent, tokenizer, model, device='cpu'):\n",
    "    #Tokenize sentence\n",
    "    encoded_input = tokenizer(sent, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "    #Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    #Perform pooling. In this case, mean pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load AutoModel from huggingface model repository OR locally cached\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/robbert\")\n",
    "model2 = AutoModelForMaskedLM.from_pretrained(\"../models/robbert\")\n",
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000])\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(df_trunc.sentence):\n",
    "    embed_rob_sent = get_embeddings(sent, tokenizer, model2, device)\n",
    "    embed_rob_stt = get_embeddings(sent, tokenizer, model2, device)\n",
    "    print(pytorch_cos_sim(embed_rob_sent, embed_rob_stt)[0].cpu())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation / Seq2Seq Inferencer\n",
    " \n",
    "Generating text with a prompt. The same setup can be extended to a seq2seq setup for training. Some relevant links are as follows:\\\n",
    "- [MBart](https://huggingface.co/docs/transformers/main/en/multilingual#mbart)\n",
    "- [Seq2Seq primer: Hugging Face](https://huggingface.co/docs/transformers/main/en/glossary#general-terms)\n",
    "- [Text Generation](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = \"../models/robbert/\"\n",
    "# modelname = \"../models/paraphrase-MiniLM-L6-v2/\"\n",
    "modelname = \"../models/bert-base-multilingual-uncased/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.\n",
      "Some weights of the model checkpoint at ../models/bert-base-multilingual-uncased/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gen_tokenizer = AutoTokenizer.from_pretrained(f\"{modelname}\")\n",
    "gen_model = AutoModelForMaskedLM.from_pretrained(f\"{modelname}\", is_decoder=True)\n",
    "gen_model = gen_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Het verslag legt nadruk op het\"\n",
    "input_ids = gen_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['het verslag legt nadruk op het. toespraak was voor, en niet niet de grond opspraakje, die niet bij de tweede']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "outputs = gen_model.generate(input_ids, do_sample=True, max_length=30)\n",
    "gen_tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The text generation isn't that great but the idea is that given a pretrained LM we can use it out of the box for fine tuning on our desired use case. I foresee following approach:\n",
    "\n",
    "1. Seq2Seq Approach:\n",
    "Using a pre-trained encoder (such as any Roberta Based moodel for Dutch) and a pre-trained decoder (any GPT 2 based model for Dutch) set up the problem statement of generating tgt seq (manual annotations) given src seq (stt output)\n",
    "    1.1 Maybe the tutorial here (https://huggingface.co/docs/transformers/tasks/translation) helps.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('hack': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40fb8cf722fc9864fe7e290b1085f1f487375054cba042a00382fef7e973f8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
